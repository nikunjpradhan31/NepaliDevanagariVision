{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "855e4b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import onnxruntime as rt\n",
    "#import torchvision.transforms as transforms\n",
    "import cv2\n",
    "\n",
    "import onnxruntime as ort\n",
    "import torch\n",
    "#import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Tuple, List, Dict, Any\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d0907eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29af097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox(img, new_shape=(2048, 2048), color=(114, 114, 114)):\n",
    "    orig_w, orig_h = img.size\n",
    "    r = min(new_shape[0] / orig_h, new_shape[1] / orig_w)\n",
    "    new_unpad = int(orig_w * r), int(orig_h * r)\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]\n",
    "    dw /= 2\n",
    "    dh /= 2\n",
    "\n",
    "    # resize\n",
    "    img_resized = img.resize(new_unpad, Image.BILINEAR)\n",
    "    # create padded image\n",
    "    new_img = Image.new(\"RGB\", new_shape, color)\n",
    "    new_img.paste(img_resized, (int(dw), int(dh)))\n",
    "    return new_img, new_unpad[0], new_unpad[1], int(dw), int(dh), r\n",
    "\n",
    "def draw_contours_on_image(image_path, detections):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Cannot load: {image_path}\")\n",
    "\n",
    "    for det in detections:\n",
    "        mask = det[\"mask\"].astype(np.uint8)\n",
    "        box = det[\"box\"]\n",
    "        score = det[\"score\"]\n",
    "\n",
    "        # find contours\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL,\n",
    "                                       cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        color = (0, 255, 0)\n",
    "\n",
    "        # draw all contours\n",
    "        cv2.drawContours(img, contours, -1, color, 2)\n",
    "\n",
    "        # draw bounding box\n",
    "        x1, y1, x2, y2 = box\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "        cv2.putText(img,\n",
    "                    f\"{score:.2f}\",\n",
    "                    (x1, max(y1 - 5, 0)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.7,\n",
    "                    color,\n",
    "                    2)\n",
    "\n",
    "    return img\n",
    "\n",
    "def post_process_segmentation(pred, proto, mask_threshold,\n",
    "                              pad_x, pad_y,\n",
    "                              orig_w, orig_h, new_w, new_h):\n",
    "    \"\"\"\n",
    "    Convert ONNX outputs into masks + boxes in ORIGINAL IMAGE coordinates.\n",
    "    \"\"\"\n",
    "\n",
    "    pred = pred[0]               # (300, 38)\n",
    "    proto = proto[0]             # (32, 256, 256)\n",
    "\n",
    "    # remove zero rows (confidence = 0)\n",
    "    pred = pred[pred[:, 4] > 0]\n",
    "\n",
    "    if len(pred) == 0:\n",
    "        return []\n",
    "\n",
    "    boxes = pred[:, 0:4]                # x1,y1,x2,y2 in letterboxed coordinates\n",
    "    scores = pred[:, 4]\n",
    "    class_ids = pred[:, 5].astype(int)\n",
    "    coeffs = pred[:, 6:]                # (N, 32)\n",
    "\n",
    "    # compute prototype masks (256×256 → 1024×1024 → unpad → orig)\n",
    "    masks = []\n",
    "    for c in coeffs:\n",
    "        m = np.tensordot(c, proto.reshape(32, -1), axes=1)\n",
    "        m = m.reshape(256, 256)\n",
    "        m = 1 / (1 + np.exp(-m))           # sigmoid\n",
    "        m = cv2.resize(m, (new_w, new_h))\n",
    "\n",
    "        # unpad\n",
    "        m = m[pad_y:pad_y+new_h, pad_x:pad_x+new_w]\n",
    "\n",
    "        # resize to original\n",
    "        m = cv2.resize(m, (orig_w, orig_h))\n",
    "        masks.append(m > mask_threshold)\n",
    "\n",
    "    # convert boxes to original-image coordinates\n",
    "    final_boxes = []\n",
    "    for b in boxes:\n",
    "        x1, y1, x2, y2 = b\n",
    "\n",
    "        # remove padding\n",
    "        x1 -= pad_x\n",
    "        x2 -= pad_x\n",
    "        y1 -= pad_y\n",
    "        y2 -= pad_y\n",
    "\n",
    "        # scale to original image\n",
    "        x1 = x1 * (orig_w / new_w)\n",
    "        x2 = x2 * (orig_w / new_w)\n",
    "        y1 = y1 * (orig_h / new_h)\n",
    "        y2 = y2 * (orig_h / new_h)\n",
    "\n",
    "        final_boxes.append([int(x1), int(y1), int(x2), int(y2)])\n",
    "\n",
    "    detections = []\n",
    "    for box, score, cls, mask in zip(final_boxes, scores, class_ids, masks):\n",
    "        detections.append({\n",
    "            \"box\": box,          # [x1,y1,x2,y2] in ORIGINAL coordinates\n",
    "            \"score\": float(score),\n",
    "            \"class\": int(cls),\n",
    "            \"mask\": mask         # binary mask in ORIGINAL resolution (orig_h×orig_w)\n",
    "        })\n",
    "\n",
    "    return detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5529a774",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_sess = rt.InferenceSession(\"models/LineDetectionv4.onnx\", providers=rt.get_available_providers())\n",
    "input_name = detection_sess.get_inputs()[0].name\n",
    "output_name = detection_sess.get_outputs()[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95ced7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open(\"images/0_page.jpeg\").convert(\"RGB\")\n",
    "orig_w, orig_h = img.size\n",
    "\n",
    "padded, new_w, new_h, pad_x, pad_y, scale = letterbox(img, new_shape=(1024,1024))\n",
    "image = np.array(padded).astype(np.float32) / 255.0\n",
    "image = image.transpose(2,0,1)[None,...]\n",
    "\n",
    "pred, proto = detection_sess.run(None, {input_name: image})\n",
    "\n",
    "detections = post_process_segmentation(\n",
    "    pred, proto, 0.5,\n",
    "    pad_x, pad_y,\n",
    "    orig_w, orig_h,\n",
    "    new_w, new_h\n",
    ")\n",
    "\n",
    "out_img = draw_contours_on_image(\"images/0_page.jpeg\", detections)\n",
    "cv2.imwrite(\"detections_output.jpg\", out_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9644b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCLabelConverter(object):\n",
    "    \"\"\" Convert between text-label and text-index \"\"\"\n",
    "\n",
    "    #def __init__(self, character, separator = []):\n",
    "    def __init__(self, character, separator_list = {}, dict_pathlist = {}):\n",
    "        # character (str): set of the possible characters.\n",
    "        dict_character = list(character)\n",
    "\n",
    "        #special_character = ['\\xa2', '\\xa3', '\\xa4','\\xa5']\n",
    "        #self.separator_char = special_character[:len(separator)]\n",
    "\n",
    "        self.dict = {}\n",
    "        #for i, char in enumerate(self.separator_char + dict_character):\n",
    "        for i, char in enumerate(dict_character):\n",
    "            # NOTE: 0 is reserved for 'blank' token required by CTCLoss\n",
    "            self.dict[char] = i + 1\n",
    "\n",
    "        self.character = ['[blank]'] + dict_character  # dummy '[blank]' token for CTCLoss (index 0)\n",
    "        #self.character = ['[blank]']+ self.separator_char + dict_character  # dummy '[blank]' token for CTCLoss (index 0)\n",
    "        self.separator_list = separator_list\n",
    "\n",
    "        separator_char = []\n",
    "        for lang, sep in separator_list.items():\n",
    "            separator_char += sep\n",
    "\n",
    "        self.ignore_idx = [0] + [i+1 for i,item in enumerate(separator_char)]\n",
    "\n",
    "        dict_list = {}\n",
    "        for lang, dict_path in dict_pathlist.items():\n",
    "            with open(dict_path, \"rb\") as input_file:\n",
    "                word_count = pickle.load(input_file)\n",
    "            dict_list[lang] = word_count\n",
    "        self.dict_list = dict_list\n",
    "\n",
    "    def encode(self, text, batch_max_length=25):\n",
    "        \"\"\"convert text-label into text-index.\n",
    "        input:\n",
    "            text: text labels of each image. [batch_size]\n",
    "\n",
    "        output:\n",
    "            text: concatenated text index for CTCLoss.\n",
    "                    [sum(text_lengths)] = [text_index_0 + text_index_1 + ... + text_index_(n - 1)]\n",
    "            length: length of each text. [batch_size]\n",
    "        \"\"\"\n",
    "        length = [len(s) for s in text]\n",
    "        text = ''.join(text)\n",
    "        text = [self.dict[char] for char in text]\n",
    "\n",
    "        return (torch.IntTensor(text), torch.IntTensor(length))\n",
    "\n",
    "    def decode_greedy(self, text_index, length):\n",
    "        \"\"\" convert text-index into text-label. \"\"\"\n",
    "        texts = []\n",
    "        index = 0\n",
    "        for l in length:\n",
    "            t = text_index[index:index + l]\n",
    "\n",
    "            char_list = []\n",
    "            for i in range(l):\n",
    "                if t[i] not in self.ignore_idx and (not (i > 0 and t[i - 1] == t[i])):  # removing repeated characters and blank (and separator).\n",
    "                #if (t[i] != 0) and (not (i > 0 and t[i - 1] == t[i])):  # removing repeated characters and blank (and separator).\n",
    "                    char_list.append(self.character[t[i]])\n",
    "            text = ''.join(char_list)\n",
    "\n",
    "            texts.append(text)\n",
    "            index += l\n",
    "        return texts\n",
    "\n",
    "    def decode_beamsearch(self, mat, beamWidth=5):\n",
    "        texts = []\n",
    "\n",
    "        for i in range(mat.shape[0]):\n",
    "            t = ctcBeamSearch(mat[i], self.character, self.ignore_idx, None, beamWidth=beamWidth)\n",
    "            texts.append(t)\n",
    "        return texts\n",
    "\n",
    "    def decode_wordbeamsearch(self, mat, beamWidth=5):\n",
    "        texts = []\n",
    "        argmax = np.argmax(mat, axis = 2)\n",
    "        for i in range(mat.shape[0]):\n",
    "            words = word_segmentation(argmax[i])\n",
    "            string = ''\n",
    "            for word in words:\n",
    "                matrix = mat[i, word[1][0]:word[1][1]+1,:]\n",
    "                if word[0] == '': dict_list = []\n",
    "                else: dict_list = self.dict_list[word[0]]\n",
    "                t = ctcBeamSearch(matrix, self.character, self.ignore_idx, None, beamWidth=beamWidth, dict_list=dict_list)\n",
    "                string += t\n",
    "            texts.append(string)\n",
    "        return texts\n",
    "\n",
    "class AttnLabelConverter(object):\n",
    "    \"\"\" Convert between text-label and text-index \"\"\"\n",
    "\n",
    "    def __init__(self, character):\n",
    "        # character (str): set of the possible characters.\n",
    "        # [GO] for the start token of the attention decoder. [s] for end-of-sentence token.\n",
    "        list_token = ['[GO]', '[s]']  # ['[s]','[UNK]','[PAD]','[GO]']\n",
    "        list_character = list(character)\n",
    "        self.character = list_token + list_character\n",
    "\n",
    "        self.dict = {}\n",
    "        for i, char in enumerate(self.character):\n",
    "            # print(i, char)\n",
    "            self.dict[char] = i\n",
    "\n",
    "    def encode(self, text, batch_max_length=25):\n",
    "        \"\"\" convert text-label into text-index.\n",
    "        input:\n",
    "            text: text labels of each image. [batch_size]\n",
    "            batch_max_length: max length of text label in the batch. 25 by default\n",
    "\n",
    "        output:\n",
    "            text : the input of attention decoder. [batch_size x (max_length+2)] +1 for [GO] token and +1 for [s] token.\n",
    "                text[:, 0] is [GO] token and text is padded with [GO] token after [s] token.\n",
    "            length : the length of output of attention decoder, which count [s] token also. [3, 7, ....] [batch_size]\n",
    "        \"\"\"\n",
    "        length = [len(s) + 1 for s in text]  # +1 for [s] at end of sentence.\n",
    "        # batch_max_length = max(length) # this is not allowed for multi-gpu setting\n",
    "        batch_max_length += 1\n",
    "        # additional +1 for [GO] at first step. batch_text is padded with [GO] token after [s] token.\n",
    "        batch_text = torch.LongTensor(len(text), batch_max_length + 1).fill_(0)\n",
    "        for i, t in enumerate(text):\n",
    "            text = list(t)\n",
    "            text.append('[s]')\n",
    "            text = [self.dict[char] for char in text]\n",
    "            batch_text[i][1:1 + len(text)] = torch.LongTensor(text)  # batch_text[:, 0] = [GO] token\n",
    "        return (batch_text.to(device), torch.IntTensor(length).to(device))\n",
    "\n",
    "    def decode(self, text_index, length):\n",
    "        \"\"\" convert text-index into text-label. \"\"\"\n",
    "        texts = []\n",
    "        for index, l in enumerate(length):\n",
    "            text = ''.join([self.character[i] for i in text_index[index, :]])\n",
    "            texts.append(text)\n",
    "        return texts\n",
    "    \n",
    "def CRNN_center_and_resize_image(img, target_size=(1220, 80)):\n",
    "    \"\"\"\n",
    "    Resize the image to fit inside target_size while maintaining aspect ratio.\n",
    "    If the image is smaller, center it on a black background of target_size.\n",
    "    \"\"\"\n",
    "    if isinstance(img, str):\n",
    "       img = Image.open(img)\n",
    "    target_w, target_h = target_size\n",
    "    img_w, img_h = img.size\n",
    "\n",
    "    if img_w > target_w or img_h > target_h:\n",
    "        img.thumbnail((target_w, target_h), Image.LANCZOS)\n",
    "    new_img = Image.new(\"RGB\", (target_w, target_h), color=\"black\")\n",
    "    paste_x = (target_w - img.width) // 2\n",
    "    paste_y = (target_h - img.height) // 2\n",
    "    new_img.paste(img, (paste_x, paste_y))\n",
    "\n",
    "    return new_img\n",
    "\n",
    "def CRNN_preprocess_image(image_path, config, return_tensor=True):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for the text recognition model.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to image or PIL Image\n",
    "        config: Model configuration object\n",
    "        return_tensor: Whether to return PyTorch tensor or PIL Image\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed image tensor or PIL Image\n",
    "    \"\"\"\n",
    "    input_channels = 3\n",
    "    \n",
    "    processed_pil = CRNN_center_and_resize_image(image_path, (config[0], config[1]))\n",
    "    \n",
    "    if not return_tensor:\n",
    "        return processed_pil\n",
    "    \n",
    "    image_np = np.array(processed_pil)\n",
    "    \n",
    "    image_np = image_np.astype(np.float32) / 255.0\n",
    "    \n",
    "    # if getattr(config, 'contrast_adjust', False):\n",
    "    #     image_np = (image_np - np.mean(image_np)) / (np.std(image_np) + 1e-8)\n",
    "    \n",
    "    if input_channels == 1:\n",
    "        image_tensor = torch.from_numpy(image_np).unsqueeze(0) \n",
    "        image_tensor = (image_tensor - 0.5) / 0.5\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        image_tensor = transform(processed_pil)\n",
    "    \n",
    "    # Add batch dimension: (1, C, H, W)\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "    \n",
    "    return image_tensor\n",
    "\n",
    "def CRNN_load_onnx_model(onnx_path):\n",
    "    \"\"\"Load ONNX model as a runtime session.\"\"\"\n",
    "    session = ort.InferenceSession(onnx_path)\n",
    "    return session\n",
    "\n",
    "def CRNN_inference_onnx_model(session, converter, image_path):\n",
    "    batch_max_length = 200\n",
    "    character = \"\"\"०१२३४५६७८९0123456789!\"#$%&'()*+,-./:;<=>?@[\\]^_`{}~।॥—‘’“”… अआइईउऊऋएऐओऔअंअःकखगघङचछजझञटठडढणतथदधनपफबभमयरलवशषसहक्षत्रज्ञािीुूृेैोौंःँॅॉ\"\"\"\n",
    "    # Preprocess image\n",
    "    image_tensor = CRNN_preprocess_image(image_path, (1220,80))  # [1, C, H, W]\n",
    "    image_np = image_tensor.cpu().numpy().astype(np.float32)\n",
    "\n",
    "    batch_size = image_np.shape[0]\n",
    "    text_input = np.zeros((batch_size, batch_max_length), dtype=np.int64)\n",
    "\n",
    "    # Run ONNX inference\n",
    "    outputs = session.run(None, {\"input\": image_np })\n",
    "    preds = outputs[0]  # [batch, seq_len, num_classes]\n",
    "\n",
    "    if converter == \"CTC\":\n",
    "        preds_size = np.array([preds.shape[1]] * batch_size, dtype=np.int32)\n",
    "        preds_index = preds.argmax(axis=2).flatten()\n",
    "        converter_class = CTCLabelConverter(character)\n",
    "        preds_str = converter_class.decode_greedy(preds_index, preds_size)\n",
    "        return preds_str[0]\n",
    "\n",
    "    elif converter == \"Attn\":\n",
    "        preds = preds[:, :batch_max_length - 1, :]\n",
    "        preds_index = preds.argmax(axis=2)\n",
    "        converter_class = AttnLabelConverter(character)\n",
    "        preds_str = converter_class.decode(torch.from_numpy(preds_index), \n",
    "                                     torch.IntTensor([batch_max_length] * batch_size))\n",
    "\n",
    "        preds_prob = F.softmax(torch.from_numpy(preds), dim=2)\n",
    "        preds_max_prob, _ = preds_prob.max(dim=2)\n",
    "\n",
    "        final_result = []\n",
    "        for pred, pred_max_prob in zip(preds_str, preds_max_prob):\n",
    "            eos_pos = pred.find('[s]')\n",
    "            if eos_pos != -1:\n",
    "                pred = pred[:eos_pos]\n",
    "                pred_max_prob = pred_max_prob[:eos_pos]\n",
    "            final_result.append(pred)\n",
    "        return final_result\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "726c644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_session = CRNN_load_onnx_model(\"models/ResNetBiLSTMCTCv1.onnx\")\n",
    "\n",
    "all_entries = os.listdir(\"crops\")\n",
    "files_only = [os.path.join(\"crops\", entry) for entry in all_entries if os.path.isfile(os.path.join(\"crops\", entry))]\n",
    "\n",
    "for index, string in enumerate(files_only):\n",
    "    if index == 5:\n",
    "        break\n",
    "    #visualize_crnn_preprocessing(string, config)\n",
    "    print(CRNN_inference_onnx_model(onnx_session, \"CTC\", string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "726c644b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_crops/crop_0.jpg यसका कुनै कर्मचारीले माग गरेको जुनसुकै कागजपत्र तथा जानकारी उपलब्ध गराउनु\n",
      "temp_crops/crop_1.jpg \"कानून बमोजिम नियमितता, मितव्ययिता, कार्यदक्षता, प्रभावकारिता र औचित्य समेतको\"\n",
      "temp_crops/crop_10.jpg \"सर्वोच्च अदालत, संघीय संसद, प्रदेश सभा, प्रदेश सरकार, स्थानीय. तह, संवैधानिक निकाय\"\n",
      "temp_crops/crop_11.jpg ९्/{्/{्0.14{्/001001001551010.80{.10\n",
      "temp_crops/crop_12.jpg सरकारको स्वामित्व भएको सङ्गठित संस्थाकोलेखापरीक्षणका लागि लेखापरीक्षक नियुक्त\n",
      "temp_crops/crop_13.jpg कुनै कार्यालय वा संस्थाको महालेखा परीक्षकबाट लेखापरीक्षण गर्नु पर्ने गरी संघीय\n",
      "temp_crops/crop_14.jpg (५) उपधारा (१) मा उल्लेख भएका कार्यालयहरूको लेखाका अतिरिक्त अन्य\n",
      "temp_crops/crop_15.jpg तर चरम आर्थिक विश्वृंखलताका कारण सङ्कटकाल घोषणा भएको अवस्थामा यो\n",
      "temp_crops/crop_16.jpg महालेखा परीक्षकले तोकेको ढाँचामा राखिनेछ ।\n",
      "temp_crops/crop_17.jpg . मुहालेखा_परीक्षाकको_काम,कर्तव्य_र्_आधिकार: (१) राष्ट्रपति र उपराष्ट्रपतिको कार्यालय,\"\n",
      "temp_crops/crop_18.jpg (ड) महालेखा परीक्षक भइसकेको व्यक्ति अन्य सरकारी सेवामा नियुक्तिका लागि\n",
      "temp_crops/crop_19.jpg (३) महालेखा परीक्षकलाई उपधारा (१) बमोजिमको कामका लागि लेखा\n",
      "temp_crops/crop_2.jpg (२) पचास प्रतिशतभन्दा बढी शेयर वा जायजेथामा नेपाल सरकार वा प्रदेश\n",
      "temp_crops/crop_20.jpg विचार गरी महालेखा परीक्षकबाट लेखापरीक्षण हुनेछ ।\n",
      "temp_crops/crop_21.jpg \"तर कुनै राजनीतिक पदमा वा कुनै विषयको अनुसन्धान, जाँचबुझ वा छानबीन गर्ने\"\n",
      "temp_crops/crop_22.jpg कानून बमोजिम व्यवस्था गर्न सकिनेछ ।\n",
      "temp_crops/crop_23.jpg व्यवस्था लागू हुने छैन ।\n",
      "temp_crops/crop_24.jpg त्रा कुनै विषयको अध्ययन वा अन्वेषण गरी राय, मन्तव्य वा सिफारिस पेश गर्ने कुनै\"\n",
      "temp_crops/crop_25.jpg सम्बन्धित कार्यालय प्रमुखको कर्तव्य हुनेछ ।\n",
      "temp_crops/crop_26.jpg सक्नेछ ।\n",
      "temp_crops/crop_27.jpg छैन ।\n",
      "temp_crops/crop_28.jpg ११७\n",
      "temp_crops/crop_29.jpg ग्राहृय हुने छैन ।\n",
      "temp_crops/crop_3.jpg १र्दा अपनाउनु पर्ने सिद्धान्तको सम्बन्धमा महालेखा परीक्षकले आवश्यक निर्देशन दिन\n",
      "temp_crops/crop_30.jpg परीक्षण\n",
      "temp_crops/crop_4.jpg गर्दा महालेखा परीक्षकसँग परामर्श गरिनेछ । त्यस्तो सङ्गठित संस्थाको लेखापरीक्षण\n",
      "temp_crops/crop_5.jpg (४) उपधारा (१) बमोजिम लेखापरीक्षण गरिने लेखा संघीय कानून बमोजिम\n",
      "temp_crops/crop_6.jpg \"वा सोको कार्यालय, अदालत, महान्यायाधिवक्ताको कार्यालय र नेपाली सेना, नेपाल प्रहरी\"\n",
      "temp_crops/crop_7.jpg \"वा सशस्त्र प्रहरी बल, नेपाल लगायतका सबै संघीय र प्रदेश सरकारी कार्यालयको लेखा\"\n",
      "temp_crops/crop_8.jpg सम्बन्धी कागजपत्र जुनसुकै बखत हेर्न पाउने अधिकार हुनेछ । महालेखा परीक्षक वा\n",
      "temp_crops/crop_9.jpg पदमा नियुक्त भई काम गर्न यस उपधारामा लेखिएको कुनै कुराले बाधा पुर्याएको मानिने\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "os.makedirs(\"temp_crops\", exist_ok=True)\n",
    "image_path = \"images/828_page.jpg\"\n",
    "detection_sess = rt.InferenceSession(\"models/LineDetectionv4.onnx\", providers=rt.get_available_providers())\n",
    "input_name = detection_sess.get_inputs()[0].name\n",
    "output_name = detection_sess.get_outputs()[0].name\n",
    "\n",
    "img = Image.open(image_path).convert(\"RGB\")\n",
    "orig_w, orig_h = img.size\n",
    "\n",
    "padded, new_w, new_h, pad_x, pad_y, scale = letterbox(img, new_shape=(1024,1024))\n",
    "image = np.array(padded).astype(np.float32) / 255.0\n",
    "image = image.transpose(2,0,1)[None,...]\n",
    "\n",
    "pred, proto = detection_sess.run(None, {input_name: image})\n",
    "\n",
    "detections = post_process_segmentation(\n",
    "    pred, proto, 0.5,\n",
    "    pad_x, pad_y,\n",
    "    orig_w, orig_h,\n",
    "    new_w, new_h\n",
    ")\n",
    "\n",
    "detection_boxes = [det[\"box\"] for det in detections]\n",
    "out_img = draw_contours_on_image(image_path, detections)\n",
    "cv2.imwrite(\"detections_output.jpg\", out_img)\n",
    "\n",
    "for i, box in enumerate(detection_boxes):\n",
    "    x1, y1, x2, y2 = box\n",
    "    x1 = max(0, x1 - 100)\n",
    "    y1 = max(0, y1 - 15)\n",
    "    x2 = min(orig_w, x2 + 100)\n",
    "    y2 = min(orig_h, y2 + 15)\n",
    "    crop = img.crop((x1, y1, x2, y2))\n",
    "\n",
    "    crop_path = os.path.join(\"temp_crops\", f\"crop_{i}.jpg\")\n",
    "    crop.save(crop_path)\n",
    "\n",
    "onnx_session = CRNN_load_onnx_model(\"models/ResNetBiLSTMCTCv1.onnx\")\n",
    "\n",
    "all_entries = os.listdir(\"temp_crops\")\n",
    "files_only = [\n",
    "    os.path.join(\"temp_crops\", entry)\n",
    "    for entry in all_entries\n",
    "    if os.path.isfile(os.path.join(\"temp_crops\", entry))\n",
    "]\n",
    "files_only.sort()\n",
    "\n",
    "for index, string in enumerate(files_only):\n",
    "    # if index == 5:\n",
    "    #     break\n",
    "    # visualize_crnn_preprocessing(string, config)\n",
    "    print(string, CRNN_inference_onnx_model(onnx_session, \"CTC\", string))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OCRTrainer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
