{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "855e4b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import onnxruntime as rt\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "\n",
    "import onnxruntime as ort\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Tuple, List, Dict, Any\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b0176b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d0907eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e29af097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox(img, new_shape=(2048, 2048), color=(114, 114, 114)):\n",
    "    orig_w, orig_h = img.size\n",
    "    r = min(new_shape[0] / orig_h, new_shape[1] / orig_w)\n",
    "    new_unpad = int(orig_w * r), int(orig_h * r)\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]\n",
    "    dw /= 2\n",
    "    dh /= 2\n",
    "\n",
    "    # resize\n",
    "    img_resized = img.resize(new_unpad, Image.BILINEAR)\n",
    "    # create padded image\n",
    "    new_img = Image.new(\"RGB\", new_shape, color)\n",
    "    new_img.paste(img_resized, (int(dw), int(dh)))\n",
    "    return new_img, new_unpad[0], new_unpad[1], int(dw), int(dh), r\n",
    "\n",
    "def draw_contours_on_image(image_path, detections):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Cannot load: {image_path}\")\n",
    "\n",
    "    for det in detections:\n",
    "        mask = det[\"mask\"].astype(np.uint8)\n",
    "        box = det[\"box\"]\n",
    "        score = det[\"score\"]\n",
    "\n",
    "        # find contours\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL,\n",
    "                                       cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        color = (0, 255, 0)\n",
    "\n",
    "        # draw all contours\n",
    "        cv2.drawContours(img, contours, -1, color, 2)\n",
    "\n",
    "        # draw bounding box\n",
    "        x1, y1, x2, y2 = box\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "        cv2.putText(img,\n",
    "                    f\"{score:.2f}\",\n",
    "                    (x1, max(y1 - 5, 0)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.7,\n",
    "                    color,\n",
    "                    2)\n",
    "\n",
    "    return img\n",
    "\n",
    "def post_process_segmentation(pred, proto, mask_threshold,\n",
    "                              pad_x, pad_y,\n",
    "                              orig_w, orig_h, new_w, new_h):\n",
    "    \"\"\"\n",
    "    Convert ONNX outputs into masks + boxes in ORIGINAL IMAGE coordinates.\n",
    "    \"\"\"\n",
    "\n",
    "    pred = pred[0]               # (300, 38)\n",
    "    proto = proto[0]             # (32, 256, 256)\n",
    "\n",
    "    # remove zero rows (confidence = 0)\n",
    "    pred = pred[pred[:, 4] > 0]\n",
    "\n",
    "    if len(pred) == 0:\n",
    "        return []\n",
    "\n",
    "    boxes = pred[:, 0:4]                # x1,y1,x2,y2 in letterboxed coordinates\n",
    "    scores = pred[:, 4]\n",
    "    class_ids = pred[:, 5].astype(int)\n",
    "    coeffs = pred[:, 6:]                # (N, 32)\n",
    "\n",
    "    # compute prototype masks (256×256 → 1024×1024 → unpad → orig)\n",
    "    masks = []\n",
    "    for c in coeffs:\n",
    "        m = np.tensordot(c, proto.reshape(32, -1), axes=1)\n",
    "        m = m.reshape(256, 256)\n",
    "        m = 1 / (1 + np.exp(-m))           # sigmoid\n",
    "        m = cv2.resize(m, (new_w, new_h))\n",
    "\n",
    "        # unpad\n",
    "        m = m[pad_y:pad_y+new_h, pad_x:pad_x+new_w]\n",
    "\n",
    "        # resize to original\n",
    "        m = cv2.resize(m, (orig_w, orig_h))\n",
    "        masks.append(m > mask_threshold)\n",
    "\n",
    "    # convert boxes to original-image coordinates\n",
    "    final_boxes = []\n",
    "    for b in boxes:\n",
    "        x1, y1, x2, y2 = b\n",
    "\n",
    "        # remove padding\n",
    "        x1 -= pad_x\n",
    "        x2 -= pad_x\n",
    "        y1 -= pad_y\n",
    "        y2 -= pad_y\n",
    "\n",
    "        # scale to original image\n",
    "        x1 = x1 * (orig_w / new_w)\n",
    "        x2 = x2 * (orig_w / new_w)\n",
    "        y1 = y1 * (orig_h / new_h)\n",
    "        y2 = y2 * (orig_h / new_h)\n",
    "\n",
    "        final_boxes.append([int(x1), int(y1), int(x2), int(y2)])\n",
    "\n",
    "    detections = []\n",
    "    for box, score, cls, mask in zip(final_boxes, scores, class_ids, masks):\n",
    "        detections.append({\n",
    "            \"box\": box,          # [x1,y1,x2,y2] in ORIGINAL coordinates\n",
    "            \"score\": float(score),\n",
    "            \"class\": int(cls),\n",
    "            \"mask\": mask         # binary mask in ORIGINAL resolution (orig_h×orig_w)\n",
    "        })\n",
    "\n",
    "    return detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5529a774",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_sess = rt.InferenceSession(\"models/LineDetectionv4.onnx\", providers=rt.get_available_providers())\n",
    "input_name = detection_sess.get_inputs()[0].name\n",
    "output_name = detection_sess.get_outputs()[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95ced7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open(\"images/0_page.jpeg\").convert(\"RGB\")\n",
    "orig_w, orig_h = img.size\n",
    "\n",
    "padded, new_w, new_h, pad_x, pad_y, scale = letterbox(img, new_shape=(1024,1024))\n",
    "image = np.array(padded).astype(np.float32) / 255.0\n",
    "image = image.transpose(2,0,1)[None,...]\n",
    "\n",
    "pred, proto = detection_sess.run(None, {input_name: image})\n",
    "\n",
    "detections = post_process_segmentation(\n",
    "    pred, proto, 0.5,\n",
    "    pad_x, pad_y,\n",
    "    orig_w, orig_h,\n",
    "    new_w, new_h\n",
    ")\n",
    "\n",
    "out_img = draw_contours_on_image(\"images/0_page.jpeg\", detections)\n",
    "cv2.imwrite(\"detections_output.jpg\", out_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9644b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCLabelConverter(object):\n",
    "    \"\"\" Convert between text-label and text-index \"\"\"\n",
    "\n",
    "    #def __init__(self, character, separator = []):\n",
    "    def __init__(self, character, separator_list = {}, dict_pathlist = {}):\n",
    "        # character (str): set of the possible characters.\n",
    "        dict_character = list(character)\n",
    "\n",
    "        #special_character = ['\\xa2', '\\xa3', '\\xa4','\\xa5']\n",
    "        #self.separator_char = special_character[:len(separator)]\n",
    "\n",
    "        self.dict = {}\n",
    "        #for i, char in enumerate(self.separator_char + dict_character):\n",
    "        for i, char in enumerate(dict_character):\n",
    "            # NOTE: 0 is reserved for 'blank' token required by CTCLoss\n",
    "            self.dict[char] = i + 1\n",
    "\n",
    "        self.character = ['[blank]'] + dict_character  # dummy '[blank]' token for CTCLoss (index 0)\n",
    "        #self.character = ['[blank]']+ self.separator_char + dict_character  # dummy '[blank]' token for CTCLoss (index 0)\n",
    "        self.separator_list = separator_list\n",
    "\n",
    "        separator_char = []\n",
    "        for lang, sep in separator_list.items():\n",
    "            separator_char += sep\n",
    "\n",
    "        self.ignore_idx = [0] + [i+1 for i,item in enumerate(separator_char)]\n",
    "\n",
    "        dict_list = {}\n",
    "        for lang, dict_path in dict_pathlist.items():\n",
    "            with open(dict_path, \"rb\") as input_file:\n",
    "                word_count = pickle.load(input_file)\n",
    "            dict_list[lang] = word_count\n",
    "        self.dict_list = dict_list\n",
    "\n",
    "    def encode(self, text, batch_max_length=25):\n",
    "        \"\"\"convert text-label into text-index.\n",
    "        input:\n",
    "            text: text labels of each image. [batch_size]\n",
    "\n",
    "        output:\n",
    "            text: concatenated text index for CTCLoss.\n",
    "                    [sum(text_lengths)] = [text_index_0 + text_index_1 + ... + text_index_(n - 1)]\n",
    "            length: length of each text. [batch_size]\n",
    "        \"\"\"\n",
    "        length = [len(s) for s in text]\n",
    "        text = ''.join(text)\n",
    "        text = [self.dict[char] for char in text]\n",
    "\n",
    "        return (torch.IntTensor(text), torch.IntTensor(length))\n",
    "\n",
    "    def decode_greedy(self, text_index, length):\n",
    "        \"\"\" convert text-index into text-label. \"\"\"\n",
    "        texts = []\n",
    "        index = 0\n",
    "        for l in length:\n",
    "            t = text_index[index:index + l]\n",
    "\n",
    "            char_list = []\n",
    "            for i in range(l):\n",
    "                if t[i] not in self.ignore_idx and (not (i > 0 and t[i - 1] == t[i])):  # removing repeated characters and blank (and separator).\n",
    "                #if (t[i] != 0) and (not (i > 0 and t[i - 1] == t[i])):  # removing repeated characters and blank (and separator).\n",
    "                    char_list.append(self.character[t[i]])\n",
    "            text = ''.join(char_list)\n",
    "\n",
    "            texts.append(text)\n",
    "            index += l\n",
    "        return texts\n",
    "\n",
    "    def decode_beamsearch(self, mat, beamWidth=5):\n",
    "        texts = []\n",
    "\n",
    "        for i in range(mat.shape[0]):\n",
    "            t = ctcBeamSearch(mat[i], self.character, self.ignore_idx, None, beamWidth=beamWidth)\n",
    "            texts.append(t)\n",
    "        return texts\n",
    "\n",
    "    def decode_wordbeamsearch(self, mat, beamWidth=5):\n",
    "        texts = []\n",
    "        argmax = np.argmax(mat, axis = 2)\n",
    "        for i in range(mat.shape[0]):\n",
    "            words = word_segmentation(argmax[i])\n",
    "            string = ''\n",
    "            for word in words:\n",
    "                matrix = mat[i, word[1][0]:word[1][1]+1,:]\n",
    "                if word[0] == '': dict_list = []\n",
    "                else: dict_list = self.dict_list[word[0]]\n",
    "                t = ctcBeamSearch(matrix, self.character, self.ignore_idx, None, beamWidth=beamWidth, dict_list=dict_list)\n",
    "                string += t\n",
    "            texts.append(string)\n",
    "        return texts\n",
    "\n",
    "class AttnLabelConverter(object):\n",
    "    \"\"\" Convert between text-label and text-index \"\"\"\n",
    "\n",
    "    def __init__(self, character):\n",
    "        # character (str): set of the possible characters.\n",
    "        # [GO] for the start token of the attention decoder. [s] for end-of-sentence token.\n",
    "        list_token = ['[GO]', '[s]']  # ['[s]','[UNK]','[PAD]','[GO]']\n",
    "        list_character = list(character)\n",
    "        self.character = list_token + list_character\n",
    "\n",
    "        self.dict = {}\n",
    "        for i, char in enumerate(self.character):\n",
    "            # print(i, char)\n",
    "            self.dict[char] = i\n",
    "\n",
    "    def encode(self, text, batch_max_length=25):\n",
    "        \"\"\" convert text-label into text-index.\n",
    "        input:\n",
    "            text: text labels of each image. [batch_size]\n",
    "            batch_max_length: max length of text label in the batch. 25 by default\n",
    "\n",
    "        output:\n",
    "            text : the input of attention decoder. [batch_size x (max_length+2)] +1 for [GO] token and +1 for [s] token.\n",
    "                text[:, 0] is [GO] token and text is padded with [GO] token after [s] token.\n",
    "            length : the length of output of attention decoder, which count [s] token also. [3, 7, ....] [batch_size]\n",
    "        \"\"\"\n",
    "        length = [len(s) + 1 for s in text]  # +1 for [s] at end of sentence.\n",
    "        # batch_max_length = max(length) # this is not allowed for multi-gpu setting\n",
    "        batch_max_length += 1\n",
    "        # additional +1 for [GO] at first step. batch_text is padded with [GO] token after [s] token.\n",
    "        batch_text = torch.LongTensor(len(text), batch_max_length + 1).fill_(0)\n",
    "        for i, t in enumerate(text):\n",
    "            text = list(t)\n",
    "            text.append('[s]')\n",
    "            text = [self.dict[char] for char in text]\n",
    "            batch_text[i][1:1 + len(text)] = torch.LongTensor(text)  # batch_text[:, 0] = [GO] token\n",
    "        return (batch_text.to(device), torch.IntTensor(length).to(device))\n",
    "\n",
    "    def decode(self, text_index, length):\n",
    "        \"\"\" convert text-index into text-label. \"\"\"\n",
    "        texts = []\n",
    "        for index, l in enumerate(length):\n",
    "            text = ''.join([self.character[i] for i in text_index[index, :]])\n",
    "            texts.append(text)\n",
    "        return texts\n",
    "    \n",
    "def CRNN_center_and_resize_image(img, target_size=(1220, 80)):\n",
    "    \"\"\"\n",
    "    Resize the image to fit inside target_size while maintaining aspect ratio.\n",
    "    If the image is smaller, center it on a black background of target_size.\n",
    "    \"\"\"\n",
    "    if isinstance(img, str):\n",
    "       img = Image.open(img)\n",
    "    target_w, target_h = target_size\n",
    "    img_w, img_h = img.size\n",
    "\n",
    "    if img_w > target_w or img_h > target_h:\n",
    "        img.thumbnail((target_w, target_h), Image.LANCZOS)\n",
    "    new_img = Image.new(\"RGB\", (target_w, target_h), color=\"black\")\n",
    "    paste_x = (target_w - img.width) // 2\n",
    "    paste_y = (target_h - img.height) // 2\n",
    "    new_img.paste(img, (paste_x, paste_y))\n",
    "\n",
    "    return new_img\n",
    "\n",
    "def CRNN_preprocess_image(image_path, config, return_tensor=True):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for the text recognition model.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to image or PIL Image\n",
    "        config: Model configuration object\n",
    "        return_tensor: Whether to return PyTorch tensor or PIL Image\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed image tensor or PIL Image\n",
    "    \"\"\"\n",
    "    input_channels = 3\n",
    "    \n",
    "    processed_pil = CRNN_center_and_resize_image(image_path, (config[0], config[1]))\n",
    "    \n",
    "    if not return_tensor:\n",
    "        return processed_pil\n",
    "    \n",
    "    image_np = np.array(processed_pil)\n",
    "    \n",
    "    image_np = image_np.astype(np.float32) / 255.0\n",
    "    \n",
    "    # if getattr(config, 'contrast_adjust', False):\n",
    "    #     image_np = (image_np - np.mean(image_np)) / (np.std(image_np) + 1e-8)\n",
    "    \n",
    "    if input_channels == 1:\n",
    "        image_tensor = torch.from_numpy(image_np).unsqueeze(0) \n",
    "        image_tensor = (image_tensor - 0.5) / 0.5\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        image_tensor = transform(processed_pil)\n",
    "    \n",
    "    # Add batch dimension: (1, C, H, W)\n",
    "    image_tensor = image_tensor.unsqueeze(0)\n",
    "    \n",
    "    return image_tensor\n",
    "\n",
    "def CRNN_load_onnx_model(onnx_path):\n",
    "    \"\"\"Load ONNX model as a runtime session.\"\"\"\n",
    "    session = ort.InferenceSession(onnx_path)\n",
    "    return session\n",
    "\n",
    "def CRNN_inference_onnx_model(session, converter, image_path):\n",
    "    batch_max_length = 200\n",
    "    character = \"\"\"०१२३४५६७८९0123456789!\"#$%&'()*+,-./:;<=>?@[\\]^_`{}~।॥—‘’“”… अआइईउऊऋएऐओऔअंअःकखगघङचछजझञटठडढणतथदधनपफबभमयरलवशषसहक्षत्रज्ञािीुूृेैोौंःँॅॉ\"\"\"\n",
    "    # Preprocess image\n",
    "    image_tensor = CRNN_preprocess_image(image_path, (1220,80))  # [1, C, H, W]\n",
    "    image_np = image_tensor.cpu().numpy().astype(np.float32)\n",
    "\n",
    "    batch_size = image_np.shape[0]\n",
    "    text_input = np.zeros((batch_size, batch_max_length), dtype=np.int64)\n",
    "\n",
    "    # Run ONNX inference\n",
    "    outputs = session.run(None, {\"input\": image_np })\n",
    "    preds = outputs[0]  # [batch, seq_len, num_classes]\n",
    "\n",
    "    if converter == \"CTC\":\n",
    "        preds_size = np.array([preds.shape[1]] * batch_size, dtype=np.int32)\n",
    "        preds_index = preds.argmax(axis=2).flatten()\n",
    "        converter_class = CTCLabelConverter(character)\n",
    "        preds_str = converter_class.decode_greedy(preds_index, preds_size)\n",
    "        return preds_str[0]\n",
    "\n",
    "    elif converter == \"Attn\":\n",
    "        preds = preds[:, :batch_max_length - 1, :]\n",
    "        preds_index = preds.argmax(axis=2)\n",
    "        converter_class = AttnLabelConverter(character)\n",
    "        preds_str = converter_class.decode(torch.from_numpy(preds_index), \n",
    "                                     torch.IntTensor([batch_max_length] * batch_size))\n",
    "\n",
    "        preds_prob = F.softmax(torch.from_numpy(preds), dim=2)\n",
    "        preds_max_prob, _ = preds_prob.max(dim=2)\n",
    "\n",
    "        final_result = []\n",
    "        for pred, pred_max_prob in zip(preds_str, preds_max_prob):\n",
    "            eos_pos = pred.find('[s]')\n",
    "            if eos_pos != -1:\n",
    "                pred = pred[:eos_pos]\n",
    "                pred_max_prob = pred_max_prob[:eos_pos]\n",
    "            final_result.append(pred)\n",
    "        return final_result\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "726c644b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "११७\n",
      "भेडीचोक\n",
      "विषय वा प्रसड.गले अन्य अर्थ नलागेमा यस विधानमा :\n",
      "\"छोरी, तिमी मुसासँग\"\n",
      "‘सभाको बैठक बसेको साठी दिनपछि स्वत: निष्क्रिय हनेछ ।१\n"
     ]
    }
   ],
   "source": [
    "onnx_session = CRNN_load_onnx_model(\"models/ResNetBiLSTMCTCv1.onnx\")\n",
    "\n",
    "all_entries = os.listdir(\"images/crops\")\n",
    "files_only = [os.path.join(\"images/crops\", entry) for entry in all_entries if os.path.isfile(os.path.join(\"images/crops\", entry))]\n",
    "\n",
    "for index, string in enumerate(files_only):\n",
    "    if index == 5:\n",
    "        break\n",
    "    #visualize_crnn_preprocessing(string, config)\n",
    "    print(CRNN_inference_onnx_model(onnx_session, \"CTC\", string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "726c644b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_crops/crop_4.jpg (ङ) वस्तुभाउलाई घाँस चाहिदैन ।\n",
      "temp_crops/crop_0.jpg (ख) पहिरो रायो भने माटोको नास हुन्छ ।६::*,\n",
      "temp_crops/crop_13.jpg (उ) रूखबिरुवा भयो भने गाउँ ........................\n",
      "temp_crops/crop_16.jpg \"हुन्छ, भन्ने आत्मज्ञान किताब लेख्दालेख्दै पाएँ ।\"\n",
      "temp_crops/crop_11.jpg १३६\n",
      "temp_crops/crop_6.jpg (क) बिरुवा रोप्ने कामलाई …......................... भनिन्छ ।’\n",
      "temp_crops/crop_8.jpg (ख) रूखबिरुवा भएन भने पानीको ........... सुक्छ ।\n",
      "temp_crops/crop_7.jpg (क) - वृक्षारोपण भनेको रूख काट्नु हो । रड्::::\n",
      "temp_crops/crop_10.jpg (ग) वनको हेरचाह -गर्ने मानिस ….......….......... हो ।\n",
      "temp_crops/crop_5.jpg (ग) बनपालेले वनको रक्षा,गर्छ ।\n",
      "temp_crops/crop_2.jpg (घ) वन नभए घाँसदाउरा पाइँंदैन ।\n",
      "temp_crops/crop_3.jpg एुरूरूरूरूरहुपहु्रहूहुहुहुँ कीक भएको वाक्य मान कापीमा सार .\n",
      "temp_crops/crop_1.jpg हुँरुुह््हुह[हुर्ुहु्जँ खाली- ठाउँमा मिल्ने शब्नद लेख _\n",
      "temp_crops/crop_15.jpg आफैलाई हेरेर आफूनो साइजमा बाँच्ने बानी परेको छ । नैतिकवान्\n",
      "temp_crops/crop_12.jpg (ङ) रूखबिरुवा भयो भने गाउँ ..................... हुन्छ ।\n",
      "temp_crops/crop_17.jpg सोचको तीसौै\n",
      "temp_crops/crop_9.jpg (घ) रूख नभएको ठाउँमा वर्षामा ................ जान्छ ।\n",
      "temp_crops/crop_14.jpg ( एलञ्साच’ पुस्तकको पहिलो प्रकाशन वि.सं. २०६१ असोज २ए\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "os.makedirs(\"temp_crops\", exist_ok=True)\n",
    "image_path = \"images/202_page.jpg\"\n",
    "detection_sess = rt.InferenceSession(\"models/LineDetectionv4.onnx\", providers=rt.get_available_providers())\n",
    "input_name = detection_sess.get_inputs()[0].name\n",
    "output_name = detection_sess.get_outputs()[0].name\n",
    "\n",
    "img = Image.open(image_path).convert(\"RGB\")\n",
    "orig_w, orig_h = img.size\n",
    "\n",
    "padded, new_w, new_h, pad_x, pad_y, scale = letterbox(img, new_shape=(1024,1024))\n",
    "image = np.array(padded).astype(np.float32) / 255.0\n",
    "image = image.transpose(2,0,1)[None,...]\n",
    "\n",
    "pred, proto = detection_sess.run(None, {input_name: image})\n",
    "\n",
    "detections = post_process_segmentation(\n",
    "    pred, proto, 0.5,\n",
    "    pad_x, pad_y,\n",
    "    orig_w, orig_h,\n",
    "    new_w, new_h\n",
    ")\n",
    "\n",
    "detection_boxes = [det[\"box\"] for det in detections]\n",
    "out_img = draw_contours_on_image(image_path, detections)\n",
    "cv2.imwrite(\"detections_output.jpg\", out_img)\n",
    "\n",
    "for i, box in enumerate(detection_boxes):\n",
    "    x1, y1, x2, y2 = box\n",
    "    x1 = max(0, x1 - 100)\n",
    "    y1 = max(0, y1 - 15)\n",
    "    x2 = min(orig_w, x2 + 100)\n",
    "    y2 = min(orig_h, y2 + 15)\n",
    "    crop = img.crop((x1, y1, x2, y2))\n",
    "\n",
    "    crop_path = os.path.join(\"temp_crops\", f\"crop_{i}.jpg\")\n",
    "    crop.save(crop_path)\n",
    "\n",
    "onnx_session = CRNN_load_onnx_model(\"models/ResNetBiLSTMCTCv1.onnx\")\n",
    "\n",
    "all_entries = os.listdir(\"temp_crops\")\n",
    "files_only = [\n",
    "    os.path.join(\"temp_crops\", entry)\n",
    "    for entry in all_entries\n",
    "    if os.path.isfile(os.path.join(\"temp_crops\", entry))\n",
    "]\n",
    "\n",
    "for index, string in enumerate(files_only):\n",
    "    # if index == 5:\n",
    "    #     break\n",
    "    # visualize_crnn_preprocessing(string, config)\n",
    "    print(string, CRNN_inference_onnx_model(onnx_session, \"CTC\", string))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OCRTrainer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
